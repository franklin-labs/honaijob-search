<!--
README du projet HonaÃ¯Job-search.
Ce fichier dÃ©crit le fonctionnement de lâ€™outil dÃ©fini dans main.py (crawler sÃ©mantique asynchrone).
-->

# HonaÃ¯Job-search 1.0

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/)
[![Async](https://img.shields.io/badge/Async-asyncio%20%2B%20aiohttp-5c8df6)](#-fonctionnement)
[![Search](https://img.shields.io/badge/Search-DuckDuckGo-DD6829)](#-fonctionnement)
[![LLM](https://img.shields.io/badge/LLM-aucun_utilisÃ©-success)](#-fonctionnement-sans-llm)
[![Status](https://img.shields.io/badge/Status-Prototype-orange)](#-defauts-actuels)

HonaÃ¯Job-search est un **prototype de crawler sÃ©mantique asynchrone** pour explorer des
**offres dâ€™emploi / stages tech** Ã  partir dâ€™une simple requÃªte texte (Python).

Il interroge DuckDuckGo, tÃ©lÃ©charge les pages en parallÃ¨le, extrait le texte, calcule une
similaritÃ© sÃ©mantique avec la requÃªte, puis affiche les rÃ©sultats les plus pertinents
dans un tableau richement formatÃ© dans le terminal.

---

## Navigation

Utilisez ces â€œongletsâ€ cliquables pour naviguer rapidement dans le README :

| Navigation | |
|-----------|--|
| [Accueil](#accueil) | [Fonctionnement](#fonctionnement) |
| [Installation](#installation) | [Utilisation](#utilisation) |
| [Architecture](#architecture-technique) | [DÃ©fis rencontrÃ©s](#dÃ©fis-rencontrÃ©s) |
| [DÃ©fauts actuels](#analyse-des-dÃ©fauts-actuels) | [Perspectives dâ€™amÃ©lioration](#perspectives-damÃ©lioration) |

---

## Accueil

HonaÃ¯Job-search 1.0 est un outil en ligne de commande qui :

- rÃ©cupÃ¨re une liste dâ€™URLs via DuckDuckGo (librairie **`ddgs`**),
- tÃ©lÃ©charge les pages de maniÃ¨re asynchrone avec **`aiohttp`**,
- extrait le contenu textuel avec **`BeautifulSoup`**,
- encode la requÃªte utilisateur et le contenu avec **`sentence-transformers`**,
- calcule une **similaritÃ© cosinus** entre la requÃªte et chaque page,
- applique un filtrage mÃ©tier (mots-clÃ©s emploi / compÃ©tences / contrat),
- affiche un **tableau colorÃ©** des rÃ©sultats dans le terminal via **`rich`**.

Usage principal : trouver rapidement des pages web susceptibles de contenir des
**offres dâ€™emploi ou de stage tech** liÃ©es Ã  une requÃªte donnÃ©e (ville, techno, type de contrat, etc.).

---

## Fonctionnement

### Vue dâ€™ensemble

Le cÅ“ur du systÃ¨me se trouve dans [`main.py`](./main.py) et repose sur :

- une classe **`EmbeddingModel`** pour gÃ©nÃ©rer des embeddings de phrases,
- une structure **`QueryIntent`** pour reprÃ©senter lâ€™intention de la requÃªte,
- une structure **`SemanticResult`** pour stocker les rÃ©sultats scorÃ©s,
- une classe **`HonaÃ¯JobCrawler`** qui orchestre la recherche,
- une fonction **`main()`** qui gÃ¨re lâ€™interface CLI et lâ€™affichage.

Flux global :

1. Lâ€™utilisateur saisit une requÃªte dans le terminal.
2. Le crawler interroge DuckDuckGo pour obtenir une liste dâ€™URLs.
3. Les pages sont tÃ©lÃ©chargÃ©es en parallÃ¨le (asyncio + aiohttp).
4. Le texte est extrait (paragraphes, listes, titresâ€¦).
5. La requÃªte et chaque page sont encodÃ©es en vecteurs via `SentenceTransformer`.
6. Une **similaritÃ© cosinus** et des **heuristiques de mots-clÃ©s** sont calculÃ©es.
7. Les rÃ©sultats sont triÃ©s par score dÃ©croissant.
8. Un tableau synthÃ©tique est affichÃ© dans le terminal (titre, URL, score, contrat, skills, extrait).

### Fonctionnement sans LLM

Lâ€™outil **ne fait appel Ã  aucun LLM externe** (type ChatGPT / GPT-4, API cloud, etc.).

Ã€ la place, dans la version actuelle, il utilise :

- un **modÃ¨le dâ€™embedding local** (`sentence-transformers/all-MiniLM-L6-v2`) chargÃ© via
  `SentenceTransformer`,
- des **mots-clÃ©s mÃ©tier** chargÃ©s depuis un fichier `keywords.json`,
- des **heuristiques de scoring** simples basÃ©es sur la similaritÃ© cosinus et la prÃ©sence de mots-clÃ©s.

#### Mots-clÃ©s mÃ©tier (keywords.json)

Le fichier `keywords.json` contient des listes de mots-clÃ©s qui dÃ©crivent votre â€œmÃ©tierâ€ ou votre contexte
de recherche (emploi, technologies, lieux, types de contrat, compÃ©tences, etc.).

Au dÃ©marrage, ces listes sont chargÃ©es en mÃ©moire et converties en ensembles Python :

- `EMPLOYMENT_KEYWORDS` : mots liÃ©s Ã  lâ€™emploi et aux offres (emploi, offre, stage, alternanceâ€¦),
- `TECH_JOB_KEYWORDS` : mots dÃ©crivant des postes ou domaines techniques (dÃ©veloppeur, backend, dataâ€¦),
- `LOCATION_KEYWORDS` : villes ou contextes gÃ©ographiques (paris, lyon, remoteâ€¦),
- `TIME_KEYWORDS` : termes liÃ©s au temps ou Ã  la fraÃ®cheur (aujourdâ€™hui, rÃ©cent, nouveauâ€¦),
- `SKILL_KEYWORDS` : compÃ©tences techniques (python, django, react, sqlâ€¦),
- `CONTRACT_KEYWORDS` : types de contrat (cdi, cdd, freelance, stage, alternance, intÃ©rimâ€¦).

Ces ensembles sont ensuite utilisÃ©s pour :

- interprÃ©ter lâ€™intention de la requÃªte (fonction `infer_query_intent`),
- filtrer les pages qui ne parlent pas du tout dâ€™emploi ou de compÃ©tences,
- dÃ©tecter et afficher, pour chaque page, les compÃ©tences et le type de contrat repÃ©rÃ©s.

Pourquoi ce choix ?

- âœ… **IndÃ©pendance vis-Ã -vis des API externes** : pas de clÃ© API, pas de coÃ»t variable.
- âœ… **ReproductibilitÃ©** : Ã  environnement Ã©gal, mÃªme requÃªte â‡’ mÃªmes scores.
- âœ… **SimplicitÃ© de dÃ©ploiement** : tout tourne en local, dans un simple script Python.
- âœ… **ContrÃ´le explicite du comportement** : pas de gÃ©nÃ©ration de texte â€œboÃ®te noireâ€.

Limitations de cette approche sans LLM :

- âŒ **ComprÃ©hension limitÃ©e** : pas de raisonnement complexe ni de reformulation fine.
- âŒ **Pas de rÃ©sumÃ© ni dâ€™extraction avancÃ©e** : seule une portion brute de texte est affichÃ©e.
- âŒ **Adaptation au domaine rÃ©duite** : la pertinence dÃ©pend fortement de la qualitÃ© des mots-clÃ©s dans `keywords.json`.

### Algorithmes et mÃ©thodes utilisÃ©es

1. **Recherche de pages**  
   - Utilisation de **`ddgs.DDGS().text(...)`** pour exÃ©cuter une recherche DuckDuckGo.
   - Filtrage des URLs internes DuckDuckGo.

2. **TÃ©lÃ©chargement asynchrone**  
   - CrÃ©ation dâ€™une **`aiohttp.ClientSession`**.
   - Lancement concurrent de requÃªtes HTTP via `asyncio.gather`.
   - Timeout par requÃªte (15s) et gestion des erreurs rÃ©seau avec logs.

3. **Extraction de texte**  
   - Parsing HTML avec **`BeautifulSoup`**.
   - Extraction du texte sur les balises `p`, `li`, `article`, `h1`, `h2`, `h3`.
   - Nettoyage du texte (espaces, sÃ©parateurs).

4. **PrÃ©traitement et embeddings**
   - Normalisation du texte : minuscules, suppression des accents (`unicodedata`), trimming.
   - Tokenisation simple alphanumÃ©rique.
   - Encodage des textes (requÃªte + contenu tronquÃ©) en vecteurs Ã  lâ€™aide de
     **`SentenceTransformer`** exÃ©cutÃ© dans un thread via `asyncio.to_thread`.

5. **Scoring sÃ©mantique**
   - Calcul de la **similaritÃ© cosinus** avec **`numpy`** entre vecteurs requÃªte / page.
   - Calcul dâ€™un ratio de **mots-clÃ©s de la requÃªte prÃ©sents dans la page**.
   - DÃ©tection de termes de rÃ©cence (`"24h"`, `"aujourd'hui"`, `"hier"`, `"rÃ©cent"`, `"nouveau"`).
   - Score final combinÃ© :

     ```text
     score = 0.5 * similaritÃ©_cosinus
           + 0.4 * ratio_mots_clÃ©s
           + 0.1 * bonus_si_date_rÃ©cente
     ```

6. **DÃ©tection de compÃ©tences et type de contrat**
   - DÃ©tection de **skills** : intersection entre les tokens de la page et `SKILL_KEYWORDS`.
   - DÃ©tection de **contrat** : premier token prÃ©sent dans `CONTRACT_KEYWORDS`.

7. **Affichage**
   - Construction dâ€™un tableau `rich.Table` :
     - colonnes : `Titre`, `URL`, `Score`, `Contrat`, `Skills`, `Extrait`.
   - Affichage dans une `rich.Console`.

---

## Installation

### PrÃ©requis

- Python **3.10+** (recommandÃ©)
- AccÃ¨s rÃ©seau sortant (pour interroger DuckDuckGo et tÃ©lÃ©charger les pages)

### Cloner le projet

```bash
git clone https://github.com/franklin-labs/honaijob-search.git
cd honaijob-search/crawller
```

### (Optionnel) CrÃ©er un environnement virtuel

```bash
python -m venv .venv

# Linux / macOS
source .venv/bin/activate

# Windows (PowerShell)
.venv\Scripts\Activate.ps1
```

### Installer les dÃ©pendances

Installation directe :

```bash
pip install aiohttp ddgs sentence-transformers numpy beautifulsoup4 rich
```

Vous pouvez Ã©galement utiliser un `requirements.txt` (Ã  complÃ©ter si besoin) :

```bash
pip install -r requirements.txt
```

### Configurer les mots-clÃ©s (keywords.json)

Ã€ la racine du dossier `crawller`, crÃ©er un fichier `keywords.json` si ce nâ€™est pas dÃ©jÃ  fait :

```json
{
  "employment_keywords": ["emploi", "offre", "stage", "alternance"],
  "tech_job_keywords": ["dÃ©veloppeur", "backend", "frontend", "data", "python"],
  "location_keywords": ["paris", "lyon", "remote"],
  "time_keywords": ["aujourd'hui", "hier", "rÃ©cent", "nouveau"],
  "skill_keywords": ["python", "django", "react", "sql"],
  "contract_keywords": ["cdi", "cdd", "freelance", "temps plein", "temps partiel", "stage", "alternance", "intÃ©rim"]
}
```

Ce fichier alimente les constantes :

- `EMPLOYMENT_KEYWORDS`
- `TECH_JOB_KEYWORDS`
- `LOCATION_KEYWORDS`
- `TIME_KEYWORDS`
- `SKILL_KEYWORDS`
- `CONTRACT_KEYWORDS`

chargÃ©es au dÃ©marrage du script.

---

## Utilisation

### Lancer le crawler

Depuis le dossier `crawller` :

```bash
python main.py
```

Le programme :

1. Demande une requÃªte de recherche dans le terminal.
2. Si vous laissez la ligne vide, il utilisera par dÃ©faut :  
   `offre de stage Ã©tudiant Paris python`.
3. Lance la recherche, tÃ©lÃ©charge les pages, calcule les scores.
4. Affiche un tableau triÃ© par pertinence.

### Exemple de requÃªtes

```text
offre de stage Ã©tudiant Paris python
alternance data engineer lyon
emploi dÃ©veloppeur backend remote django
```

### Exemple de sortie (simplifiÃ©e)

```text
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                  RÃ©sultats de recherche pour : offre stage python           â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•£
â•‘ Titre        â”‚ URL                         â”‚ Score â”‚ Contratâ”‚ Skillsâ”‚ Extrait
â•Ÿâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â•¢
â•‘ Stage Python â”‚ https://exemple.com/offreâ€¦  â”‚ 0.87  â”‚ stage  â”‚ pythonâ”‚ ...   â”‚
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•
```

> Remarque : le script Ã©crit Ã©galement quelques informations dans la sortie standard
> (`print(title)`, `print(url)`), ce qui permet un debug rapide en plus du tableau `rich`.

---

## Architecture technique

### Vue textuelle

```text
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   Utilisateur CLI  â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚ requÃªte texte
                          â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚     main.main()    â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚ instancie
                          â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  HonaÃ¯JobCrawler   â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                 â–¼                  â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ _search_syncâ”‚   â”‚ _fetch_page   â”‚   â”‚ _extract_text     â”‚
 â”‚ (DDGS)      â”‚   â”‚ (aiohttp)     â”‚   â”‚ (BeautifulSoup)   â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                 â”‚                  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â–¼
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚ EmbeddingModel â”‚
                            â”‚ (SentenceTrans.)â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â–¼
                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                           â”‚ _compute_score   â”‚
                           â”‚ + _detect_skills â”‚
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â–¼
                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                           â”‚ SemanticResult   â”‚
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â–¼
                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                           â”‚  Affichage rich  â”‚
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Principales classes / fonctions

- `EmbeddingModel` : encapsule `SentenceTransformer` pour produire des embeddings en tÃ¢che de fond.
- `QueryIntent` / `infer_query_intent(...)` : prÃ©pare lâ€™intention de la requÃªte (domaine, lieux, skills).
- `SemanticResult` : structure de donnÃ©es pour les rÃ©sultats scorÃ©s.
- `HonaÃ¯JobCrawler` :
  - `_search_sync` : interroge DuckDuckGo,
  - `_fetch_page` : tÃ©lÃ©charge les pages (async),
  - `_extract_text` : rÃ©cupÃ¨re le texte utile,
  - `_detect_skills_contract` : repÃ¨re compÃ©tences et type de contrat,
  - `_compute_score` : combine similaritÃ© cosinus, match de mots-clÃ©s, rÃ©cence,
  - `search` : mÃ©thode principale orchestrant lâ€™ensemble.
- `main()` :
  - lit la requÃªte utilisateur,
  - appelle `HonaÃ¯JobCrawler.search`,
  - construit et affiche le tableau `rich`.

---

## DÃ©fis rencontrÃ©s

Quelques dÃ©fis clÃ© lors du dÃ©veloppement :

- **Asynchronisme rÃ©seau**  
  GÃ©rer plusieurs tÃ©lÃ©chargements HTTP en parallÃ¨le tout en restant robuste aux timeouts
  et aux erreurs (sites inaccessibles, HTML mal formÃ©, etc.).

- **Extraction de texte gÃ©nÃ©rique**  
  Les pages dâ€™offres sont trÃ¨s hÃ©tÃ©rogÃ¨nes (CMS, mise en page, publicitÃ©s). Lâ€™extraction
  basÃ©e sur quelques balises (`p`, `li`, `article`, `h1`, `h2`, `h3`) est un compromis
  entre simplicitÃ© et efficacitÃ©, mais ne couvre pas tous les cas.

- **Pertinence sans LLM**  
  Obtenir des rÃ©sultats utilisables en se limitant Ã  :
  - des embeddings statiques,
  - un score simple,
  - des mots-clÃ©s fournis par lâ€™utilisateur (fichier JSON).

- **Performance des embeddings**  
  MÃªme si le modÃ¨le `all-MiniLM-L6-v2` est lÃ©ger, encoder un grand nombre de pages
  reste coÃ»teux. Dâ€™oÃ¹ le choix de tronquer le contenu (`text[:1500]`) et dâ€™exÃ©cuter
  lâ€™encodage dans un thread sÃ©parÃ© (`asyncio.to_thread`).

- **DÃ©tection de compÃ©tences et de contrats**  
  La dÃ©tection purement lexicale (intersection de sets) est sensible :
  - Ã  la qualitÃ© du texte extrait,
  - au vocabulaire exact utilisÃ© par les sites.

---

## Analyse des dÃ©fauts actuels

Lâ€™outil est un **prototype** et prÃ©sente des limitations importantes :

- **FiabilitÃ© de lâ€™extraction HTML**  
  - certains sites peuvent renvoyer du contenu vide ou obfusquÃ©,
  - lâ€™outil peut ignorer des offres si le texte est dans des balises non couvertes.

- **Scoring heuristique simpliste**
  - le score est une combinaison linÃ©aire Ã  3 termes (`similaritÃ©`, `keyword_match`, `date_rÃ©cente`),
  - aucun apprentissage supervisÃ© nâ€™est utilisÃ©,
  - des rÃ©sultats peu pertinents peuvent malgrÃ© tout Ãªtre bien scorÃ©s si les mots-clÃ©s
    sont prÃ©sents plusieurs fois.

- **Gestion limitÃ©e des erreurs**
  - absence de gestion fine des codes HTTP (403, 429, etc.),
  - pas de retry / backoff automatique,
  - pas de limite stricte de concurrence ou de rate limiting.

- **Pas de configuration externe avancÃ©e**
  - les poids de scoring sont codÃ©s en dur,
  - les paramÃ¨tres (timeout, nombre de rÃ©sultats, liste de balises HTML Ã  analyser)
    ne sont pas exposÃ©s via arguments CLI ou fichier de configuration.

- **Pas de tests automatisÃ©s**
  - absence de tests unitaires / dâ€™intÃ©gration,
  - aucune garantie de non-rÃ©gression lors des modifications.

- **Affichage parfois verbeux**
  - Ã  la fois des `print()` et un tableau `rich` (double sortie),
  - pas de mode â€œsilencieuxâ€ ou â€œdebugâ€ paramÃ©trable.

---

## Perspectives dâ€™amÃ©lioration

### PrioritÃ© haute

- **1. Rendre la configuration flexible**
  - exposer les paramÃ¨tres clÃ©s via des options CLI (`argparse`),
  - introduire un fichier `config.toml` ou `yaml` pour les rÃ©glages par dÃ©faut,
  - permettre de dÃ©finir plusieurs profils de recherche (stage, CDI, freelanceâ€¦).

- **2. AmÃ©liorer le scoring**
  - ajouter des signaux : longueur du texte, prÃ©sence dâ€™email / tÃ©lÃ©phone, mots-clÃ©s nÃ©gatifs,
  - expÃ©rimenter des variantes du score (logarithmes, normalisation par taille de page),
  - ajouter un petit module de rÃ©ordonnancement (re-ranking) basÃ© sur des rÃ¨gles.

- **3. Renforcer la robustesse rÃ©seau**
  - implÃ©menter un systÃ¨me de retry exponentiel,
  - plafonner le nombre de requÃªtes parallÃ¨les,
  - journaliser plus finement les erreurs par domaine.

- **4. Introduire un petit modÃ¨le local pour remplacer `keywords.json`**
  - concevoir un **petit modÃ¨le de classification ou de tagging**, lÃ©ger, capable de tourner sur CPU,
  - lâ€™entraÃ®ner Ã  partir dâ€™exemples annotÃ©s (requÃªtes, extraits de pages) pour prÃ©dire :
    - le domaine (emploi / tech / autre),
    - les compÃ©tences principales,
    - Ã©ventuellement le type de contrat,
  - utiliser ce modÃ¨le pour remplacer ou complÃ©ter :
    - lâ€™interprÃ©tation de lâ€™intention (`infer_query_intent`),
    - la dÃ©tection de compÃ©tences et de contrat,
  - conserver la philosophie du projet :
    - modÃ¨le **local**, sans dÃ©pendance Ã  une API distante,
    - suffisamment lÃ©ger pour tourner sur un **CPU standard**, mÃªme sur une machine modeste.

### PrioritÃ© moyenne

- **4. QualitÃ© du texte extrait**
  - adapter dynamiquement les balises scannÃ©es selon le site,
  - filtrer les menus, footers, et contenus peu informatifs,
  - ajouter des heuristiques pour dÃ©tecter les sections â€œDescription du posteâ€, â€œProfil recherchÃ©â€, etc.

- **5. UX en ligne de commande**
  - proposer un mode â€œinteractifâ€ (plusieurs requÃªtes successives dans la mÃªme session),
  - ajouter une option pour exporter les rÃ©sultats (CSV / JSON),
  - amÃ©lioration du design du tableau (troncature conditionnelle, colonnes optionnelles).

### PrioritÃ© basse

- **6. Industrialisation**
  - packager lâ€™outil en module installable (`pip install honaijob-search`),
  - fournir une image Docker prÃªte Ã  lâ€™emploi,
  - intÃ©grer un systÃ¨me de logs configurable (fichiers, JSON, niveaux).

- **7. IntÃ©grations externes**
  - permettre de pousser les rÃ©sultats vers un outil externe (Notion, Airtable, etc.),
  - exposer une petite API HTTP locale (FastAPI) pour dÃ©clencher la recherche depuis dâ€™autres services.

- **8. ExpÃ©rimenter un petit LLM local sur CPU**
  - Ã©valuer un **petit modÃ¨le de langage** (taille rÃ©duite) capable de tourner sur un PC basique uniquement avec le CPU,
  - lâ€™utiliser en option pour des tÃ¢ches ciblÃ©es :
    - re-ranking plus fin des offres les plus pertinentes,
    - gÃ©nÃ©ration de courts rÃ©sumÃ©s dâ€™offres,
    - reformulation de la requÃªte utilisateur pour explorer des variantes,
  - conserver lâ€™architecture actuelle comme base :
    - embeddings + heuristiques restent le cÅ“ur robuste et reproductible,
    - le petit LLM vient en surcouche facultative, activable/dÃ©sactivable selon les ressources de la machine.

---

## Licence MIT

Ce projet est distribuÃ© sous licence **MIT**.

Voir le texte complet de la licence dans ce dÃ©pÃ´t si nÃ©cessaire.

---

## ğŸ¤ Contributions

Les contributions sont les bienvenues (corrections de bugs, amÃ©lioration du scoring,
ajout de nouvelles heuristiques mÃ©tier, documentation).  
Proposez une _issue_ ou une _pull request_ avec une description claire de votre changement.
