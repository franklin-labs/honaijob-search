<!--
README du projet Hona√ØJob-search.
Ce fichier d√©crit le fonctionnement de l‚Äôoutil d√©fini dans main.py (crawler s√©mantique asynchrone).
-->

# Hona√ØJob-search 1.0

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/)
[![Async](https://img.shields.io/badge/Async-asyncio%20%2B%20aiohttp-5c8df6)](#-fonctionnement)
[![Search](https://img.shields.io/badge/Search-DuckDuckGo-DD6829)](#-fonctionnement)
[![LLM](https://img.shields.io/badge/LLM-aucun_utilis√©-success)](#-fonctionnement-sans-llm)
[![Status](https://img.shields.io/badge/Status-Prototype-orange)](#-defauts-actuels)

Hona√ØJob-search est un **prototype de crawler s√©mantique asynchrone** pour explorer des
**offres d‚Äôemploi / stages tech** √† partir d‚Äôune simple requ√™te texte (Python).

Il interroge DuckDuckGo, t√©l√©charge les pages en parall√®le, extrait le texte, calcule une
similarit√© s√©mantique avec la requ√™te, puis affiche les r√©sultats les plus pertinents
dans un tableau richement format√© dans le terminal.

---

## Navigation

Utilisez ces ‚Äúonglets‚Äù cliquables pour naviguer rapidement dans le README :

| Navigation | |
|-----------|--|
| [Accueil](#accueil) | [Fonctionnement](#fonctionnement) |
| [Installation](#installation) | [Utilisation](#utilisation) |
| [Architecture](#architecture-technique) | [D√©fis rencontr√©s](#d√©fis-rencontr√©s) |
| [D√©fauts actuels](#analyse-des-d√©fauts-actuels) | [Perspectives d‚Äôam√©lioration](#perspectives-dam√©lioration) |

---

## Accueil

Hona√ØJob-search 1.0 est un outil en ligne de commande qui :

- r√©cup√®re une liste d‚ÄôURLs via DuckDuckGo (librairie **`ddgs`**),
- t√©l√©charge les pages de mani√®re asynchrone avec **`aiohttp`**,
- extrait le contenu textuel avec **`BeautifulSoup`**,
- encode la requ√™te utilisateur et le contenu avec **`sentence-transformers`**,
- calcule une **similarit√© cosinus** entre la requ√™te et chaque page,
- applique un filtrage m√©tier (mots-cl√©s emploi / comp√©tences / contrat),
- affiche un **tableau color√©** des r√©sultats dans le terminal via **`rich`**.

Usage principal : trouver rapidement des pages web susceptibles de contenir des
**offres d‚Äôemploi ou de stage tech** li√©es √† une requ√™te donn√©e (ville, techno, type de contrat, etc.).

---

## Fonctionnement

### Vue d‚Äôensemble

Le c≈ìur du syst√®me se trouve dans [`main.py`](./main.py) et repose sur :

- une classe **`EmbeddingModel`** pour g√©n√©rer des embeddings de phrases,
- une structure **`QueryIntent`** pour repr√©senter l‚Äôintention de la requ√™te,
- une structure **`SemanticResult`** pour stocker les r√©sultats scor√©s,
- une classe **`Hona√ØJobCrawler`** qui orchestre la recherche,
- une fonction **`main()`** qui g√®re l‚Äôinterface CLI et l‚Äôaffichage.

Flux global :

1. L‚Äôutilisateur saisit une requ√™te dans le terminal.
2. Le crawler interroge DuckDuckGo pour obtenir une liste d‚ÄôURLs.
3. Les pages sont t√©l√©charg√©es en parall√®le (asyncio + aiohttp).
4. Le texte est extrait (paragraphes, listes, titres‚Ä¶).
5. La requ√™te et chaque page sont encod√©es en vecteurs via `SentenceTransformer`.
6. Une **similarit√© cosinus** et des **heuristiques de mots-cl√©s** sont calcul√©es.
7. Les r√©sultats sont tri√©s par score d√©croissant.
8. Un tableau synth√©tique est affich√© dans le terminal (titre, URL, score, contrat, skills, extrait).

### Fonctionnement sans LLM

L‚Äôoutil **ne fait appel √† aucun LLM externe** (type ChatGPT / GPT-4, API cloud, etc.).

√Ä la place, il utilise :

- un **mod√®le d‚Äôembedding local** (`sentence-transformers/all-MiniLM-L6-v2`) charg√© via
  `SentenceTransformer`,
- des **mots-cl√©s m√©tier** charg√©s depuis un fichier `keywords.json`,
- des **heuristiques de scoring** simples bas√©es sur la similarit√© cosinus et la pr√©sence de mots-cl√©s.

#### Mots-cl√©s m√©tier (keywords.json)

Le fichier `keywords.json` contient des listes de mots-cl√©s qui d√©crivent votre ‚Äúm√©tier‚Äù ou votre contexte
de recherche (emploi, technologies, lieux, types de contrat, comp√©tences, etc.).

Au d√©marrage, ces listes sont charg√©es en m√©moire et converties en ensembles Python :

- `EMPLOYMENT_KEYWORDS` : mots li√©s √† l‚Äôemploi et aux offres (emploi, offre, stage, alternance‚Ä¶),
- `TECH_JOB_KEYWORDS` : mots d√©crivant des postes ou domaines techniques (d√©veloppeur, backend, data‚Ä¶),
- `LOCATION_KEYWORDS` : villes ou contextes g√©ographiques (paris, lyon, remote‚Ä¶),
- `TIME_KEYWORDS` : termes li√©s au temps ou √† la fra√Æcheur (aujourd‚Äôhui, r√©cent, nouveau‚Ä¶),
- `SKILL_KEYWORDS` : comp√©tences techniques (python, django, react, sql‚Ä¶),
- `CONTRACT_KEYWORDS` : types de contrat (cdi, cdd, freelance, stage, alternance, int√©rim‚Ä¶).

Ces ensembles sont ensuite utilis√©s pour :

- interpr√©ter l‚Äôintention de la requ√™te (fonction `infer_query_intent`),
- filtrer les pages qui ne parlent pas du tout d‚Äôemploi ou de comp√©tences,
- d√©tecter et afficher, pour chaque page, les comp√©tences et le type de contrat rep√©r√©s.

Pourquoi ce choix ?

- ‚úÖ **Ind√©pendance vis-√†-vis des API externes** : pas de cl√© API, pas de co√ªt variable.
- ‚úÖ **Reproductibilit√©** : √† environnement √©gal, m√™me requ√™te ‚áí m√™mes scores.
- ‚úÖ **Simplicit√© de d√©ploiement** : tout tourne en local, dans un simple script Python.
- ‚úÖ **Contr√¥le explicite du comportement** : pas de g√©n√©ration de texte ‚Äúbo√Æte noire‚Äù.

Limitations de cette approche sans LLM :

- ‚ùå **Compr√©hension limit√©e** : pas de raisonnement complexe ni de reformulation fine.
- ‚ùå **Pas de r√©sum√© ni d‚Äôextraction avanc√©e** : seule une portion brute de texte est affich√©e.
- ‚ùå **Adaptation au domaine r√©duite** : la pertinence d√©pend fortement de la qualit√© des mots-cl√©s dans `keywords.json`.

### Algorithmes et m√©thodes utilis√©es

1. **Recherche de pages**  
   - Utilisation de **`ddgs.DDGS().text(...)`** pour ex√©cuter une recherche DuckDuckGo.
   - Filtrage des URLs internes DuckDuckGo.

2. **T√©l√©chargement asynchrone**  
   - Cr√©ation d‚Äôune **`aiohttp.ClientSession`**.
   - Lancement concurrent de requ√™tes HTTP via `asyncio.gather`.
   - Timeout par requ√™te (15s) et gestion des erreurs r√©seau avec logs.

3. **Extraction de texte**  
   - Parsing HTML avec **`BeautifulSoup`**.
   - Extraction du texte sur les balises `p`, `li`, `article`, `h1`, `h2`, `h3`.
   - Nettoyage du texte (espaces, s√©parateurs).

4. **Pr√©traitement et embeddings**
   - Normalisation du texte : minuscules, suppression des accents (`unicodedata`), trimming.
   - Tokenisation simple alphanum√©rique.
   - Encodage des textes (requ√™te + contenu tronqu√©) en vecteurs √† l‚Äôaide de
     **`SentenceTransformer`** ex√©cut√© dans un thread via `asyncio.to_thread`.

5. **Scoring s√©mantique**
   - Calcul de la **similarit√© cosinus** avec **`numpy`** entre vecteurs requ√™te / page.
   - Calcul d‚Äôun ratio de **mots-cl√©s de la requ√™te pr√©sents dans la page**.
   - D√©tection de termes de r√©cence (`"24h"`, `"aujourd'hui"`, `"hier"`, `"r√©cent"`, `"nouveau"`).
   - Score final combin√© :

     ```text
     score = 0.5 * similarit√©_cosinus
           + 0.4 * ratio_mots_cl√©s
           + 0.1 * bonus_si_date_r√©cente
     ```

6. **D√©tection de comp√©tences et type de contrat**
   - D√©tection de **skills** : intersection entre les tokens de la page et `SKILL_KEYWORDS`.
   - D√©tection de **contrat** : premier token pr√©sent dans `CONTRACT_KEYWORDS`.

7. **Affichage**
   - Construction d‚Äôun tableau `rich.Table` :
     - colonnes : `Titre`, `URL`, `Score`, `Contrat`, `Skills`, `Extrait`.
   - Affichage dans une `rich.Console`.

---

## Installation

### Pr√©requis

- Python **3.10+** (recommand√©)
- Acc√®s r√©seau sortant (pour interroger DuckDuckGo et t√©l√©charger les pages)

### Cloner le projet

```bash
git clone <votre-url-git> honaijob-search
cd honaijob-search/crawller
```

### (Optionnel) Cr√©er un environnement virtuel

```bash
python -m venv .venv

# Linux / macOS
source .venv/bin/activate

# Windows (PowerShell)
.venv\Scripts\Activate.ps1
```

### Installer les d√©pendances

Installation directe :

```bash
pip install aiohttp ddgs sentence-transformers numpy beautifulsoup4 rich
```

Vous pouvez √©galement utiliser un `requirements.txt` (√† compl√©ter si besoin) :

```bash
pip install -r requirements.txt
```

### Configurer les mots-cl√©s (keywords.json)

√Ä la racine du dossier `crawller`, cr√©er un fichier `keywords.json` si ce n‚Äôest pas d√©j√† fait :

```json
{
  "employment_keywords": ["emploi", "offre", "stage", "alternance"],
  "tech_job_keywords": ["d√©veloppeur", "backend", "frontend", "data", "python"],
  "location_keywords": ["paris", "lyon", "remote"],
  "time_keywords": ["aujourd'hui", "hier", "r√©cent", "nouveau"],
  "skill_keywords": ["python", "django", "react", "sql"],
  "contract_keywords": ["cdi", "cdd", "freelance", "temps plein", "temps partiel", "stage", "alternance", "int√©rim"]
}
```

Ce fichier alimente les constantes :

- `EMPLOYMENT_KEYWORDS`
- `TECH_JOB_KEYWORDS`
- `LOCATION_KEYWORDS`
- `TIME_KEYWORDS`
- `SKILL_KEYWORDS`
- `CONTRACT_KEYWORDS`

charg√©es au d√©marrage du script.

---

## Utilisation

### Lancer le crawler

Depuis le dossier `crawller` :

```bash
python main.py
```

Le programme :

1. Demande une requ√™te de recherche dans le terminal.
2. Si vous laissez la ligne vide, il utilisera par d√©faut :  
   `offre de stage √©tudiant Paris python`.
3. Lance la recherche, t√©l√©charge les pages, calcule les scores.
4. Affiche un tableau tri√© par pertinence.

### Exemple de requ√™tes

```text
offre de stage √©tudiant Paris python
alternance data engineer lyon
emploi d√©veloppeur backend remote django
```

### Exemple de sortie (simplifi√©e)

```text
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                  R√©sultats de recherche pour : offre stage python           ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï§‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï§‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï§‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï§‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï§‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë Titre        ‚îÇ URL                         ‚îÇ Score ‚îÇ Contrat‚îÇ Skills‚îÇ Extrait
‚ïü‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ï¢
‚ïë Stage Python ‚îÇ https://exemple.com/offre‚Ä¶  ‚îÇ 0.87  ‚îÇ stage  ‚îÇ python‚îÇ ...   ‚îÇ
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïß‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïß‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïß‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïß‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïß‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
```

> Remarque : le script √©crit √©galement quelques informations dans la sortie standard
> (`print(title)`, `print(url)`), ce qui permet un debug rapide en plus du tableau `rich`.

---

## Architecture technique

### Vue textuelle

```text
                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                ‚îÇ   Utilisateur CLI  ‚îÇ
                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚îÇ requ√™te texte
                          ‚ñº
                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                ‚îÇ     main.main()    ‚îÇ
                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚îÇ instancie
                          ‚ñº
                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                ‚îÇ  Hona√ØJobCrawler   ‚îÇ
                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚ñº                 ‚ñº                  ‚ñº
 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
 ‚îÇ _search_sync‚îÇ   ‚îÇ _fetch_page   ‚îÇ   ‚îÇ _extract_text     ‚îÇ
 ‚îÇ (DDGS)      ‚îÇ   ‚îÇ (aiohttp)     ‚îÇ   ‚îÇ (BeautifulSoup)   ‚îÇ
 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                 ‚îÇ                  ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                     ‚ñº
                            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                            ‚îÇ EmbeddingModel ‚îÇ
                            ‚îÇ (SentenceTrans.)‚îÇ
                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                     ‚ñº
                           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                           ‚îÇ _compute_score   ‚îÇ
                           ‚îÇ + _detect_skills ‚îÇ
                           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚ñº
                           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                           ‚îÇ SemanticResult   ‚îÇ
                           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚ñº
                           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                           ‚îÇ  Affichage rich  ‚îÇ
                           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Principales classes / fonctions

- `EmbeddingModel` : encapsule `SentenceTransformer` pour produire des embeddings en t√¢che de fond.
- `QueryIntent` / `infer_query_intent(...)` : pr√©pare l‚Äôintention de la requ√™te (domaine, lieux, skills).
- `SemanticResult` : structure de donn√©es pour les r√©sultats scor√©s.
- `Hona√ØJobCrawler` :
  - `_search_sync` : interroge DuckDuckGo,
  - `_fetch_page` : t√©l√©charge les pages (async),
  - `_extract_text` : r√©cup√®re le texte utile,
  - `_detect_skills_contract` : rep√®re comp√©tences et type de contrat,
  - `_compute_score` : combine similarit√© cosinus, match de mots-cl√©s, r√©cence,
  - `search` : m√©thode principale orchestrant l‚Äôensemble.
- `main()` :
  - lit la requ√™te utilisateur,
  - appelle `Hona√ØJobCrawler.search`,
  - construit et affiche le tableau `rich`.

---

## D√©fis rencontr√©s

Quelques d√©fis cl√© lors du d√©veloppement :

- **Asynchronisme r√©seau**  
  G√©rer plusieurs t√©l√©chargements HTTP en parall√®le tout en restant robuste aux timeouts
  et aux erreurs (sites inaccessibles, HTML mal form√©, etc.).

- **Extraction de texte g√©n√©rique**  
  Les pages d‚Äôoffres sont tr√®s h√©t√©rog√®nes (CMS, mise en page, publicit√©s). L‚Äôextraction
  bas√©e sur quelques balises (`p`, `li`, `article`, `h1`, `h2`, `h3`) est un compromis
  entre simplicit√© et efficacit√©, mais ne couvre pas tous les cas.

- **Pertinence sans LLM**  
  Obtenir des r√©sultats utilisables en se limitant √† :
  - des embeddings statiques,
  - un score simple,
  - des mots-cl√©s fournis par l‚Äôutilisateur (fichier JSON).

- **Performance des embeddings**  
  M√™me si le mod√®le `all-MiniLM-L6-v2` est l√©ger, encoder un grand nombre de pages
  reste co√ªteux. D‚Äôo√π le choix de tronquer le contenu (`text[:1500]`) et d‚Äôex√©cuter
  l‚Äôencodage dans un thread s√©par√© (`asyncio.to_thread`).

- **D√©tection de comp√©tences et de contrats**  
  La d√©tection purement lexicale (intersection de sets) est sensible :
  - √† la qualit√© du texte extrait,
  - au vocabulaire exact utilis√© par les sites.

---

## Analyse des d√©fauts actuels

L‚Äôoutil est un **prototype** et pr√©sente des limitations importantes :

- **Fiabilit√© de l‚Äôextraction HTML**  
  - certains sites peuvent renvoyer du contenu vide ou obfusqu√©,
  - l‚Äôoutil peut ignorer des offres si le texte est dans des balises non couvertes.

- **Scoring heuristique simpliste**
  - le score est une combinaison lin√©aire √† 3 termes (`similarit√©`, `keyword_match`, `date_r√©cente`),
  - aucun apprentissage supervis√© n‚Äôest utilis√©,
  - des r√©sultats peu pertinents peuvent malgr√© tout √™tre bien scor√©s si les mots-cl√©s
    sont pr√©sents plusieurs fois.

- **Gestion limit√©e des erreurs**
  - absence de gestion fine des codes HTTP (403, 429, etc.),
  - pas de retry / backoff automatique,
  - pas de limite stricte de concurrence ou de rate limiting.

- **Pas de configuration externe avanc√©e**
  - les poids de scoring sont cod√©s en dur,
  - les param√®tres (timeout, nombre de r√©sultats, liste de balises HTML √† analyser)
    ne sont pas expos√©s via arguments CLI ou fichier de configuration.

- **Pas de tests automatis√©s**
  - absence de tests unitaires / d‚Äôint√©gration,
  - aucune garantie de non-r√©gression lors des modifications.

- **Affichage parfois verbeux**
  - √† la fois des `print()` et un tableau `rich` (double sortie),
  - pas de mode ‚Äúsilencieux‚Äù ou ‚Äúdebug‚Äù param√©trable.

---

## Perspectives d‚Äôam√©lioration

### Priorit√© haute

- **1. Rendre la configuration flexible**
  - exposer les param√®tres cl√©s via des options CLI (`argparse`),
  - introduire un fichier `config.toml` ou `yaml` pour les r√©glages par d√©faut,
  - permettre de d√©finir plusieurs profils de recherche (stage, CDI, freelance‚Ä¶).

- **2. Am√©liorer le scoring**
  - ajouter des signaux : longueur du texte, pr√©sence d‚Äôemail / t√©l√©phone, mots-cl√©s n√©gatifs,
  - exp√©rimenter des variantes du score (logarithmes, normalisation par taille de page),
  - ajouter un petit module de r√©ordonnancement (re-ranking) bas√© sur des r√®gles.

- **3. Renforcer la robustesse r√©seau**
  - impl√©menter un syst√®me de retry exponentiel,
  - plafonner le nombre de requ√™tes parall√®les,
  - journaliser plus finement les erreurs par domaine.

### Priorit√© moyenne

- **4. Qualit√© du texte extrait**
  - adapter dynamiquement les balises scann√©es selon le site,
  - filtrer les menus, footers, et contenus peu informatifs,
  - ajouter des heuristiques pour d√©tecter les sections ‚ÄúDescription du poste‚Äù, ‚ÄúProfil recherch√©‚Äù, etc.

- **5. UX en ligne de commande**
  - proposer un mode ‚Äúinteractif‚Äù (plusieurs requ√™tes successives dans la m√™me session),
  - ajouter une option pour exporter les r√©sultats (CSV / JSON),
  - am√©lioration du design du tableau (troncature conditionnelle, colonnes optionnelles).

### Priorit√© basse

- **6. Industrialisation**
  - packager l‚Äôoutil en module installable (`pip install honaijob-search`),
  - fournir une image Docker pr√™te √† l‚Äôemploi,
  - int√©grer un syst√®me de logs configurable (fichiers, JSON, niveaux).

- **7. Int√©grations externes**
  - permettre de pousser les r√©sultats vers un outil externe (Notion, Airtable, etc.),
  - exposer une petite API HTTP locale (FastAPI) pour d√©clencher la recherche depuis d‚Äôautres services.

> Remarque : l‚Äôajout facultatif d‚Äôun LLM pour le **re-ranking** ou le **r√©sum√© d‚Äôoffres**
> pourrait faire partie d‚Äôune branche exp√©rimentale, tout en conservant le c≈ìur du
> syst√®me bas√© sur des embeddings et des heuristiques locales.

---

## Licence MIT

Ce projet est distribu√© sous licence **MIT**.

Voir le texte complet de la licence dans ce d√©p√¥t si n√©cessaire.

---

## ü§ù Contributions

Les contributions sont les bienvenues (corrections de bugs, am√©lioration du scoring,
ajout de nouvelles heuristiques m√©tier, documentation).  
Proposez une _issue_ ou une _pull request_ avec une description claire de votre changement.
